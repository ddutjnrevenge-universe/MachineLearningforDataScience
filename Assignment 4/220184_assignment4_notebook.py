# -*- coding: utf-8 -*-
"""220184-assignment4-notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cLqMR9OZoPpiFYRyth_o3ORi4FsvsWSO

# Assignment 4: Support Vector Machine (SVM) and Model Ensemble {-}

This assignment aims at familiarizing you with training and testing Suppor Vector Machine classification model, along with exploiting the power of model ensemble technics. Here are the BASIC requirements of the assignment:

- Load the data.
- Analyze the data.
- Remove outliers and clean the data.
- Use GridSearchCV to find the best set of SVM hyperparameters.
- Build, train and evaluate the SVM model.
- Separately build, train and evaluate the other four classifiers (Logistic regression, Naive Bayes, Decision Tree, Random Forest) on the same dataset, then compare their performance with the SVM model's.
- Apply three model ensemble technics, i.e., Bagging, Boosting and Stacking, to solve the problem, then compare their performance with each other and with the use of individual models. Draw conclusion from what has been observed.

The dataset you will be working on is 'data-breast-cancer.csv'. It is composed of attributes to build a prediction model.

### Submission {-}
The structure of submission folder should be organized as follows:

- ./\<StudentID>-assignment4-notebook.ipynb: Jupyter notebook containing source code.

The submission folder is named ML4DS-\<StudentID>-Assignment4 (e.g., ML4DS-2012345-Assigment4) and then compressed with the same name.
    
### Evaluation {-}
Assignment evaluation will be conducted on how you accomplish the assignment requirements. It is a plus if you have data exploration and modeling steps other than the basic requirements. In addition, your code should conform to a Python coding convention such as PEP-8.

### Deadline {-}
Please visit Canvas for details.
"""

# Load the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# Load the dataset"""

# Load the dataset
df = pd.read_csv("https://github.com/ddutjnrevenge-universe/MachineLearningforDataScience/raw/main/Assignment%204/data-breast-cancer.csv")

# Show some data samples
df.head()

"""This is a dataset used to detect whether a patient has breast cancer depending on the following features:

- diagnosis: (label) the diagnosis of breast (label) tissues (M = malignant, B = benign).
- radius: distances from center to points on the perimeter.
- texture: standard deviation of gray-scale values.
- perimeter: perimeter of the tumor.
- area: area of the tumor.
- smoothness: local variation in radius lengths.
- compactness: is equal to (perimeter^2 / area - 1.0).
- concavity: severity of concave portions of the contour.
- concave points: number of concave portions of the contour.
- symmetry: symmetry of the tumor shape.
- fractal dimension: "coastline approximation" - 1.

# Analyze the data
"""

# Your code goes here
df.info()

"""There's no null value in the dataset!"""

df.shape

# Create a copy of the dataset
data = df.copy()

# Drop the columns Unnamed:0
data.drop(columns=['Unnamed: 0'], axis = 1, inplace = True)

data

# Check for duplicated data
data.duplicated().values.any()

# Draw the histogram of the data's features
data.hist(figsize=(20,20), histtype='bar', ec='white',grid=False)
plt.show()

# Draw the boxplots of data features
data.boxplot(figsize=(20,20))
plt.show()

# Extract the numerical features
num_cols = [col for col in data.columns if data[col].dtypes == 'float64']
num_cols

# Heatmap for correlations
plt.figure(figsize=(15,15))
sns.heatmap(data[num_cols].corr(), cmap="viridis", annot=True)
plt.show()

df.describe()

# Create the dataframe with the median of every feature for each label
diagnosis_median = data.groupby('diagnosis').median()
diagnosis_median

B = diagnosis_median.loc['B'].tolist()
M = diagnosis_median.loc['M'].tolist()

x = np.arange(len(diagnosis_median.columns))
width = 0.35

fig, ax = plt.subplots(figsize=(20,10))
rects1 = ax.bar(x-width/2, B, width, label='B')
rects2 = ax.bar(x+width/2, M, width, label='M')

ax.set_xticks(x)
ax.set_xticklabels(diagnosis_median.columns)
plt.legend()
plt.show()

"""# Remove outliers & clean the data"""

# Define a function to remove outliers from a numerical column
def remove_outliers(df, col):
    q_high = df[col].quantile(0.98)  # Select upper quantile range as 98%
    q_low = df[col].quantile(0.02)   # Select lower quantile range as 2%
    df_clean = df[(df[col] < q_high) & (df[col] > q_low)]
    return df_clean

# Initialize a dataframe to store the cleaned data
df_cleaned = data.copy()

# Iterate over each numerical column and remove outliers
for col in num_cols:
    df_cleaned = remove_outliers(df_cleaned, col)

# Check the shape of the cleaned dataset
print("Shape of cleaned dataset:", df_cleaned.shape)

df_cleaned.hist(figsize=(20,20), histtype='bar', ec='white',grid=False)
plt.show()

df_cleaned.boxplot(figsize=(20,15))
plt.show()

"""# SVM Model"""

# I did try to apply the model on data_clean but the result is worse than the original dataset so I restore data_clean = data
# data_clean = df_cleaned
data_clean = data

# Separating the data features
X = data_clean.drop(columns = ['diagnosis'], axis = 1)

# Assign data labels to variable y
y = data_clean.diagnosis

# Encoding the label y
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)

# Split train/test with random_state = 10
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10, train_size = 0.8)

X_train.head()

# Initialize and use StandardScaler to normalize the data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_normalized_train = scaler.fit_transform(X_train)
X_normalized_test = scaler.transform(X_test)

# Initialize and fit the SVC model to the training set without tuning hyperparameter
from sklearn.svm import SVC
svm = SVC()
svm.fit(X_normalized_train, y_train)

# Import GridSearchCV for finding the best hyper-parameter set.
from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.01, 0.1, 1, 10, 100, 1000],
             'gamma': ['scale', 0.001, 0.005, 0.1]}
gridsearch = GridSearchCV(SVC(), param_grid, cv=10, scoring='f1', verbose=1)

# Run the search on training data samples.
svm_gs = gridsearch.fit(X_normalized_train, y_train)

# Showing the best hyperparamter set
svm_gs.best_params_

# Show the results of each hyperparameter with 10-fold cross validation
result = pd.DataFrame(svm_gs.cv_results_)
result = result.set_index('params')

# Ranking the score of each hyperparameter to choose the best one
result[['mean_test_score', 'rank_test_score']].sort_values(by=['rank_test_score'])

# Build the SVC pipeline from the best hyperparamter set
svm_cv = SVC(C=gridsearch.best_params_['C'], gamma = gridsearch.best_params_['gamma'])
svm_cv.fit(X_normalized_train, y_train)

# @title **SVM model using data removed outliers in all columns** *! DO NOT RERUN !*

cleaned_data_report_tuning = classification_report(y_test, svm_cv.predict(X_normalized_test))
cleaned_data_report_no_tuning = classification_report(y_test, svm.predict(X_normalized_test))
print("SVM model's result with tuning hyperparamter")
print(cleaned_data_report_tuning)
print("SVM model's result without tuning hyperparamter")
print(cleaned_data_report_no_tuning)

"""**I stored these reports to see that the model perform better on the original data than the data which has been removed outliers in all columns**

## SVM Model using original data
"""

# SVM model's result with tuning hyperparamter
from sklearn.metrics import classification_report
print("SVM model used original data")
print("SVM model's result with tuning hyperparamter")
print(classification_report(y_test, svm_cv.predict(X_normalized_test)))

# SVM model's result without tuning hyperparamter
print("SVM model's result without tuning hyperparamter")
print(classification_report(y_test, svm.predict(X_normalized_test)))

"""## SVM model with removing outliers from only 2 most skewed features"""

# Calculate skewness for each numerical feature
skewness = data[num_cols].skew()

# Sort skewness values in descending order
skewness = skewness.sort_values(ascending=False)

# Print skewness values
print("Skewness of numerical features:")
print(skewness)

"""**SO the most two skewed features from the dataset is "area_mean" and "concavity mean"**"""

# Removing outliers from 2 most skewed features
q = data['area_mean'].quantile(0.98)  # Select q range as 98%
p = data['area_mean'].quantile(0.02)
data_clean_trial = data[(data['area_mean'] < q)
                  & (data['area_mean'] > p)]

q = data['concavity_mean'].quantile(0.98)  # Select q range as 98%
p = data['concavity_mean'].quantile(0.02)
data_clean_trial = data_clean_trial[(data_clean_trial['concavity_mean'] < q) & (data_clean_trial['concavity_mean'] > p)]
data_clean_trial.shape

# Separate the data features and data labels
X_trial = data_clean_trial.drop(columns = ['diagnosis'], axis = 1)
y_trial = data_clean_trial.diagnosis

# Encoding the labels
y_trial = le.fit_transform(y_trial)

# Split the data samples to train/test set
X_train_trial, X_test_trial, y_train_trial, y_test_trial= train_test_split(X_trial, y_trial, random_state = 10, train_size = 0.8)

# Normalize the data
X_normalized_train_trial = scaler.fit_transform(X_train_trial)
X_normalized_test_trial = scaler.transform(X_test_trial)

# Initialize and fit the SVC model without tuning hyperparamters
svm.fit(X_normalized_train_trial, y_train_trial)

# Run the search on training data samples.
svm_gs = gridsearch.fit(X_normalized_train_trial, y_train_trial)

# Build the SVC pipeline from the best hyperparamter set
svm_cv = SVC(C=gridsearch.best_params_['C'], gamma = gridsearch.best_params_['gamma'])
svm_cv.fit(X_normalized_train_trial, y_train_trial)

print("SVM model's result without tuning hyperparamter")
print(classification_report(y_test_trial, svm.predict(X_normalized_test_trial)))
print("SVM model's result with tuning hyperparamter")
print(classification_report(y_test_trial, svm_cv.predict(X_normalized_test_trial)))

"""**Evaluation:**
* The SVM models using the original data (with and without tuning hyperparameters) generally perform better compared to the SVM model using the dataset with removed outliers in all columns.
* Removing outliers from only 2 most skewed features resulted in comparable performance to the SVM model using the original data, suggesting that focusing on outliers in those specific features may be sufficient for improving model performance.
* Overall, SVM models demonstrate high accuracy and F1-scores for both classes, indicating good performance in classifying breast cancer diagnoses.

# Four classifiers

## Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression()                      # Initialize Logistic Regression model

params = {'C': [0.001, 0.01, 0.01, 1, 10, 100]}
gridsearch = GridSearchCV(logmodel, params, cv = 10)

# # Run the search on the training set
log_gs = gridsearch.fit(X_normalized_train, y_train)
log_gs_trial = gridsearch.fit(X_normalized_train_trial, y_train_trial)

# Ranking the score of each hyperparameter to choose the best one
result = pd.DataFrame(log_gs.cv_results_)
result = result.set_index('params')
result[['mean_test_score', 'rank_test_score']].sort_values(by='rank_test_score')

# Train the model
log_model = LogisticRegression(C = log_gs.best_params_['C'])
log_model.fit(X_normalized_train, y_train)

# Train the model
log_model_trial = LogisticRegression(C = log_gs_trial.best_params_['C'])
log_model_trial.fit(X_normalized_train_trial, y_train_trial)

print('Classification report with original dataset')
print(classification_report(y_test, log_model.predict(X_normalized_test)))

print('Classification report with cleaned dataset')
print(classification_report(y_test, log_model_trial.predict(X_normalized_test)))

"""## Naive Bayes

###Without StandardScaler
"""

# Initialize Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB
naive_model = GaussianNB()

# Define the values of hyperparameter var_smoothing we want to try
grid_search={"var_smoothing":[1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15, 2e-2, 2e-3, 2e-4, 2e-5, 2e-6, 2e-7, 2e-8, 2e-9, 2e-10, 2e-11, 2e-12, 2e-13, 2e-14, 2e-15]} # Define the values of hyperparameter C we want to try

# Set up GridSearchCV to find the best value of hyperparameter var_smoothing, with 10-fold cross validation
from sklearn.model_selection import GridSearchCV
gridsearch=GridSearchCV(naive_model, grid_search, cv=10)

# Train the model
naive_gs = gridsearch.fit(X_train, y_train)
naive_gs_trial = gridsearch.fit(X_train_trial, y_train_trial)

# Show the results of each hyperparameter var_smoothing with 10-fold cross validation
result_normal = pd.DataFrame(naive_gs.cv_results_)
result_normal = result_normal.set_index('params')

# Ranking the score of each hyperparameter var_smoothing to choose the best one
result_normal[['mean_test_score', 'rank_test_score']].sort_values(by=['rank_test_score'])

# Initialize the naive-bayes model and fit
naive = GaussianNB(var_smoothing = naive_gs.best_params_['var_smoothing'])
naive.fit(X_train, y_train)

# Initialize the naive-bayes model and fit
naive_trial = GaussianNB(var_smoothing = naive_gs_trial.best_params_['var_smoothing'])
naive_trial.fit(X_train_trial, y_train_trial)

print('Classification report with original dataset')
print(classification_report(y_test, naive.predict(X_test)))

print('Classification report with cleaned dataset')
print(classification_report(y_test_trial, naive_trial.predict(X_test_trial)))

"""###With StandardScaler"""

# Run the search on training set
naive_normal_gs = gridsearch.fit(X_normalized_train, y_train)

# Initialize and train the model
naive_normal = GaussianNB(var_smoothing = naive_normal_gs.best_params_['var_smoothing'])
naive_normal.fit(X_normalized_train, y_train)

# Run the search on training set
naive_normal_gs_trial = gridsearch.fit(X_normalized_train_trial, y_train_trial)

# Initialize and train the model
naive_normal_trial = GaussianNB(var_smoothing = naive_normal_gs_trial.best_params_['var_smoothing'])
naive_normal_trial.fit(X_normalized_train_trial, y_train_trial)

print('Classification report with original dataset')
print(classification_report(y_test, naive_normal.predict(X_normalized_test)))

print('Classification report with cleaned dataset')
print(classification_report(y_test_trial, naive_normal_trial.predict(X_normalized_test_trial)))

"""##Decision Tree"""

from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier

tree = DecisionTreeClassifier()
params = {'criterion': ['gini', 'entropy'],
        "max_depth": [4, 6, 8, 10],                           # Maximum depth of the tree
         "min_samples_split": [2, 4, 6, 8],
          "min_samples_leaf": [2, 4, 6, 8],
          "max_leaf_nodes": [8, 10, 12]}

gridsearch = GridSearchCV(tree, params, cv=5)

# Run the search on the training set
tree_gs = gridsearch.fit(X_train, y_train)
tree_gs_trial = gridsearch.fit(X_train_trial, y_train_trial)

# Ranking the score of each hyperparameter to choose the best one
result = pd.DataFrame(tree_gs.cv_results_)
result = result.set_index('params')
result[['mean_test_score', 'rank_test_score']].sort_values(by=['rank_test_score'])

# Initialize the model
tree_model = DecisionTreeClassifier(criterion = tree_gs.best_params_['criterion'],
                                    max_depth = tree_gs.best_params_['max_depth'],
                                    max_leaf_nodes = tree_gs.best_params_['max_leaf_nodes'],
                                    min_samples_split = tree_gs.best_params_['min_samples_split'],
                                    min_samples_leaf = tree_gs.best_params_['min_samples_leaf'])

# Initialize the model
tree_model_trial = DecisionTreeClassifier(criterion = tree_gs_trial.best_params_['criterion'],
                                    max_depth = tree_gs_trial.best_params_['max_depth'],
                                    max_leaf_nodes = tree_gs_trial.best_params_['max_leaf_nodes'],
                                    min_samples_split = tree_gs_trial.best_params_['min_samples_split'],
                                    min_samples_leaf = tree_gs_trial.best_params_['min_samples_leaf'])

# Train the model
tree_model.fit(X_train, y_train)
tree_model_trial.fit(X_train_trial, y_train_trial)

print('Classification report with original dataset')
print(classification_report(y_test, tree_model.predict(X_test)))

print('Classification report with cleaned dataset')
print(classification_report(y_test_trial, tree_model_trial.predict(X_test_trial)))

"""##Random Forest"""

from sklearn.ensemble import RandomForestClassifier

params = {"criterion": ["gini", "entropy"],             # Criterion to evaluate the purity.
         "max_depth": [7, 9, 11],                           # Maximum depth of the tree
         "min_samples_split": [8, 12, 16]}

# Initialize the GridSeachCV
gridsearch = GridSearchCV(RandomForestClassifier(n_estimators=10, n_jobs = 10),params,cv=10)

# Run the search on the training set
forest_gs = gridsearch.fit(X_train, y_train)
forest_gs_trial = gridsearch.fit(X_train_trial, y_train_trial)

result = pd.DataFrame(forest_gs.cv_results_)
result = result.set_index('params')

result[['mean_test_score', 'rank_test_score']].sort_values(by='rank_test_score')

# Build the pipeline from the best hyperparamter set
forest_model = RandomForestClassifier(n_estimators=10,
                                      random_state=1,
                                      criterion = forest_gs.best_params_['criterion'],
                                      max_depth = forest_gs.best_params_['max_depth'],
                                      min_samples_split = forest_gs.best_params_['min_samples_split'])

# Build the pipeline from the best hyperparamter set
forest_model_trial = RandomForestClassifier(n_estimators=10,
                                      random_state=1,
                                      criterion = forest_gs_trial.best_params_['criterion'],
                                      max_depth = forest_gs_trial.best_params_['max_depth'],
                                      min_samples_split = forest_gs_trial.best_params_['min_samples_split'])

forest_model.fit(X_train, y_train)
print('Classification report with original dataset')
print(classification_report(y_test, forest_model.predict(X_test)))

forest_model_trial.fit(X_train_trial, y_train_trial)
print('Classification report with cleaned dataset')
print(classification_report(y_test_trial, forest_model_trial.predict(X_test_trial)))

"""##Comparision

Comparing these models with the SVM models' performance:

* For the original data, the SVM model generally performs similarly to or slightly better than other models in terms of accuracy and F1-scores.
* However, after removing outliers from the two most skewed features, some models like Decision Tree and Random Forest show improvement in performance, approaching or surpassing the SVM model's performance.
* Naive Bayes, especially with the cleaned data and without StandardScaler, also shows competitive performance, though slightly lower than the SVM model.

**Overall, the choice of model depends on various factors such as dataset characteristics, computational resources, interpretability, and specific goals of the analysis. However, based on these results, SVM models generally perform well and are competitive with other models.**

# Model ensemble techniques

##Bagging with SVM Machine
"""

# Bagging with Support Vector Machine (SVM)
from sklearn.ensemble import BaggingClassifier
from sklearn.svm import SVC

# Initialize base SVM model
base_svm = SVC(kernel='linear', C=1.0)

# Create Bagging Classifier with SVM as the base model
bagging_clf = BaggingClassifier(base_estimator=base_svm, n_estimators=10, max_samples=0.5)

# Train the Bagging Classifier
bagging_clf.fit(X_train, y_train)

# Evaluate the Bagging Classifier
bagging_accuracy = bagging_clf.score(X_test, y_test)
print("Bagging Classifier Accuracy:", bagging_accuracy)

"""##Boosting"""

# Boosting with AdaBoost, Gradient Boosting, and XGBoost
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier

"""###AdaBoost"""

# AdaBoost
ada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10)
ada_clf.fit(X_train, y_train)
ada_accuracy = ada_clf.score(X_test, y_test)
print("AdaBoost Classifier Accuracy:", ada_accuracy)

"""###Gradient Boosting"""

# Gradient Boosting
gb_clf = GradientBoostingClassifier(n_estimators=10, learning_rate=0.1)
gb_clf.fit(X_train, y_train)
gb_accuracy = gb_clf.score(X_test, y_test)
print("Gradient Boosting Classifier Accuracy:", gb_accuracy)

"""###XGBoost"""

# XGBoost
xgb_clf = XGBClassifier(n_estimators=100, learning_rate=0.1)
xgb_clf.fit(X_train, y_train)
xgb_accuracy = xgb_clf.score(X_test, y_test)
print("XGBoost Classifier Accuracy:", xgb_accuracy)

"""##Stacking

###K-Nearest Neighbors (KNN)
"""

# Train a K-Nearest Neighbor (KNN) model
knn = KNeighborsClassifier()                           # Initialize KNN model.
params_knn = {'n_neighbors': np.arange(1, 25)}         # n_neighbors in KNeighborsClassifier() indicates the number of neighbors K.
knn_gs = GridSearchCV(knn, params_knn, cv=5)           # Initialize GridSearchCV to find an optimal value of K.
knn_gs.fit(X_train, y_train)                           # Fit GridSearch in training set to find the optimal K.

# Best number of neighbors K
knn_best = knn_gs.best_estimator_
print(knn_gs.best_params_)

"""###Support Vector Machine (SVM)"""

# Train a Support Vector Machine (SVM) model
svm = SVC()

# C is a hyperparameter that controls the trade-off between training error and margin maximization.
# Higher C: Stricter enforcement of correct classification for all training points, and less emphasis on margin maximization.
# Lower C: More tolerance for misclassifications on the training data, and more emphasis on margin maximization.
# For more hyperparameters of SVM, kindly refer to https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
params_svm = {"C": [0.1, 1, 10, 100]}

svm_gs = GridSearchCV(svm, params_svm, cv=5)    # Initialize GridSearchCV to find an optimal value if the hyperparameter C.
svm_gs.fit(X_train, y_train)                    # Fit GridSearch in training set to find the optimal C.

# Best value of the hyperparameter C.
svm_best = svm_gs.best_estimator_
print(svm_gs.best_params_)

"""###Random Forest"""

# Train a Random Forest classifier
rf = RandomForestClassifier()                        # Initialize a Random Forest Classifier.
params_rf = {'n_estimators': [50, 100, 200]}         # n_estimator in RandomForestClassifier(...) indicates the number of Trees in the Forest.
rf_gs = GridSearchCV(rf, params_rf, cv=5)            # Initialize GridSearchCV to find an optimal number of Trees.
rf_gs.fit(X_train, y_train)                          # Fit GridSearch in training set to find the optimal number of Trees.

# Best number of Trees.
rf_best = rf_gs.best_estimator_
print(rf_gs.best_params_)

"""###Logistic Regression"""

# Train a Logistic Regression model
log_reg = LogisticRegression(solver='lbfgs', max_iter=1000)   # Initialize Logistic Regression model.
log_reg.fit(X_train, y_train)                                 # Fit the model to training set.

"""###Model Testing"""

# Print accuracy of single models on the test set
print('KNN: {}'.format(knn_best.score(X_test, y_test)))                     # KNN accuracy
print('SVM: {}'.format(svm_best.score(X_test, y_test)))                     # SVM accuracy
print('Random Forest: {}'.format(rf_best.score(X_test, y_test)))            # Random Forest accuracy
print('Logistic Regression: {}'.format(log_reg.score(X_test, y_test)))      # Logistic Regression accuracy

"""###Model Ensembling"""

# Ensemble the four models using hard (majority) voting
estimators=[('knn', knn_best), ('svm', svm_best), ('rf', rf_best), ('log_reg', log_reg)]    # Initialize base models in the ensemble
ensemble = VotingClassifier(estimators, voting='hard')                                      # Define how to ensemble them, i.e., hard voting

# Train the model ensemble on the training set
ensemble.fit(X_train, y_train)          # Train the ensemble on the training set
voting_accuracy = ensemble.score(X_test, y_test)          # Test the ensemble on the test set
print("Voting Classifier Accuracy:", voting_accuracy)

"""##Comparision & Conclusion

**Comparison:**

* Among the ensemble methods, XGBoost performs the best, achieving an accuracy of 0.965, followed by Gradient Boosting (0.930) and AdaBoost (0.904).
* Stacking with the individual models and a Voting Classifier show similar accuracies, around 0.868. This indicates that simple majority voting of these models doesn't provide significant improvement over using the individual models separately.
* Random Forest among the individual models achieves the highest accuracy of 0.947, followed by SVM (0.868), Logistic Regression (0.860), and KNN (0.851).

**Conclusion:**

* Overall, ensemble methods, particularly XGBoost, demonstrate superior performance compared to individual models and simple model stacking. They leverage the strengths of multiple models to improve predictive accuracy.
* The choice between the ensemble methods depends on various factors such as computational resources, interpretability, and the specific characteristics of the dataset. XGBoost stands out as the best-performing model in this comparison, but it may require more computational resources compared to simpler ensemble methods like AdaBoost.
"""