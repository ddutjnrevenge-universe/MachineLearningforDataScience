# -*- coding: utf-8 -*-
"""220184-assignment2-notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iDnv4attqUzA7c6DWkbNhII78XVszi91

# **Assignment 2: K-means clustering {-}**

This assignment aims at familiarizing you with training and testing K-means clustering model. You will have to:

- Load the data.
- Analyze the data.
- Use Elbow method to determine the number of clusters K. Research the use of parameters of KElbowVisualizer function.
- Train a K-means clustering model.
- Perform cluster visualization in two ways:
  - Apply PCA to perform dimensionality reduction: project data features into a three-dimensional space and two-dimensional space and visualize the clusters.
  - Visualize the clusters using T-distributed Stochastic Neighbor Embedding (T-SNE). T-SNE is a tool for visualizing high-dimensional data. T-SNE, based on stochastic neighbor embedding, is a nonlinear dimensionality reduction technique to visualize data in a two or three dimensional space. Refernece: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html
- Take some samples from each cluster and analyze their features to see the clusters' characteristics.


The dataset you will be working on is 'credit-card-holder-data.csv'. It is composed of attributes such as balance of the credit card, ammount of purchase, etc., to develop a customer segmentation model.

### Submission {-}
The structure of submission folder should be organized as follows:

- ./\<StudentID>-assignment2-notebook.ipynb: Jupyter notebook containing source code.

The submission folder is named ML4DS-\<StudentID>-Assignment2 (e.g., ML4DS-2012345-Assigment2) and then compressed with the same name.
    
### Evaluation {-}
Assignment evaluation will be conducted on how properly you handle the data for training the K-means model, evaluate the model performance, visualize customer clusters and analyze the cluster characteristics. In addition, your code should conform to a Python coding convention such as PEP-8.

### Deadline {-}
Please visit Canvas for details.
"""

# Load pandas library
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""#Load the data"""

# Load dataset in pandas dataframe
df = pd.read_csv("https://raw.githubusercontent.com/ddutjnrevenge-universe/MachineLearningforDataScience/main/credit-card-holder-data.csv")

# Show several
df.head()

"""This is a dataset for developing a customer segmentation. The dataset summarizes the usage behavior of active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables.

- BALANCE: Balance amount left in their account to make purchases.
- BALANCE_FREQUENCY: How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated).
- PURCHASES: Amount of purchases made from account.
- ONEOFF_PURCHASES: Maximum purchase amount done in one-go (i.e., one-time payment for a purchase).
- INSTALLMENTS_PURCHASES: Amount of purchase done in installment (i.e., multiple payments for a purchase). Observation: PURCHASES = ONEOFF_PURCHASE + INSTALLMENT_PURCHASES).
- CASH_ADVANCE: Cash in advance given by the user (a cash advance is when taking money out of the ATM using credit card instead of debit card or ATM card).
- PURCHASES_FREQUENCY: How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased).
- ONEOFF_PURCHASES_FREQUENCY: How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased).
- PURCHASES_INSTALLMENTS_FREQUENCY: How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done).
- CASH_ADVANCE_FREQUENCY: How frequently the cash in advance being paid.
- CASH_ADVANCE_TRX: Number of Transactions made with "Cash in Advanced".
- PURCHASES_TRX: Number of purchase transactions made.
- CREDIT_LIMIT: Limit of Credit Card for user.
- PAYMENTS: Amount of payment done by user (i.e., payment for the use of credit card)
- MINIMUM_PAYMENTS: Minimum amount of payments made by user.
- PRC_FULL_PAYMENT: Percent of full payment paid by user.
- TENURE: Tenure of credit card service for user (i.e., how long (in months) you should have held the credit card before they will grant you credit).

#Exploratory Data Analysis
"""

df.info()

"""**Observation: This info table shows that the dataset has no missing values**"""

# Check if there is any duplicated values
df.duplicated().sum()

df.describe()

# Drop unnamed column as it does not contain any useful information for building the model
df = df.drop(columns=["Unnamed: 0"], axis=1)

# Plot histograms of all data features
df.hist(figsize=(20,20))
plt.show()

# Polt the correlation heatmap between pairs of features.
plt.figure(figsize=(15,20))
sns.heatmap(df.corr(), annot=True)

"""- Observation: BALANCE feature and PRC_FULL_PAYMENT feature has a strong negative correlation, which means as one increases, the other tends to decrease, and vice versa.

In this case, when the BALANCE feature increases (indicating a higher balance amount left in the account), the PRC_FULL_PAYMENT feature tends to decrease (indicating a lower percentage of full payment made by the user).
- So do the CASH_ADVANCE_FREQUENCY and PURCHASES_FREQUENCY.

This suggests that users who frequently make cash advances might not engage in frequent purchase transactions, and vice versa.

# Data Preprocessing
"""

# Make a copy of the data
data = df.copy()

# Import StandardScaler to standardize the data features.
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# Fit (calculate mean and standard deviation) and transform (substract mean and divide by std) the data
scaled_data = scaler.fit_transform(data.values)

"""#Use Elbow method to determine the number of clusters K"""

# Import KElbowVisualizer to use Elbow method
from yellowbrick.cluster import KElbowVisualizer

# Import KMeans library to use Kmeans algorithm
from sklearn.cluster import KMeans

# Determine the number of clusters (K) using Elbow method
elbow_visualizer = KElbowVisualizer(KMeans(), k=(1, 11), timings=False, locate_elbow=True)
elbow_visualizer.fit(scaled_data)
elbow_visualizer.show()

"""# Train a K-means clustering model"""

# Based on the Elbow method, let's choose the number of clusters as 4
kmeans_model = KMeans(n_clusters=4)

# Run K-means algorithm with K=4
kmeans_model.fit(scaled_data)

# Show cluster label of training data samples
kmeans_model.labels_

# Make cluster label prediction of new data samples (assume here we use scaled_data as tes data samples).
kmeans_model.predict(scaled_data)

# Show location of cluster centroids
kmeans_model.cluster_centers_

"""#Perform cluster visualization in two ways:

##Dimensionality reduction with Principal Component Analysis (PCA)
"""

# Import PCA library
from sklearn.decomposition import PCA

# Find all principal components (i.e, eigen vectors of the covariance matrix, and eigen values) of the data, and equal to the number of data features.
pca = PCA(n_components=len(data.columns))
pca.fit(scaled_data)

# Print the amount of variance carried in each principal components (eigenvalues). This is the eigen values.
print("Amount of variance carried in each principal components (eigen values) :\n" + str(pca.explained_variance_))

# Print the ratio of ammount of variance carried in each principal components (eigen values). The ratio is calculated by dividing the eigenvalue of each component by the sum of eigenvalues
print("\nRatio of variance carried in each principal components (eigen values) :\n" + str(pca.explained_variance_ratio_))

# Sum of all ratios must be 1
print("\nCummulative sum of all ratios :\n" + str(pca.explained_variance_ratio_.cumsum()))

# Note: by ranking the eigenvectors in order of their eigenvalues, highest to lowest, we get the principal components in order of significance.

"""### 3-dimensional PCA"""

# We test the three most important principal components as they cover 56.4% (0.564) of the data variance.
pca3 = PCA(n_components=3)

# Fit (i.e., construct the 3-dimensional PCA space) and transform (i.e., project the original data points into the 3-dimensional PCA space).
pca3_data = pca3.fit_transform(scaled_data)

# Create a dataframe from the projected data points in 3-dimensional PCA space, namely "pca_1", "pca_2" and "pca_3"
pca3_df = pd.DataFrame(pca3_data, columns=["pca_1", "pca_2", "pca_3"])

# Print the amount of variance carried in each principal components (eigenvalues). This is the eigen values.
print("Amount of variance carried in each principal components (eigen values) :\n" + str(pca3.explained_variance_))

# Print the ratio of ammount of variance carried in each principal components (eigen values). The ratio is calculated by dividing the eigenvalue of each component by the sum of eigenvalues
print("\nRatio of variance carried in each principal components (eigen values) :\n" + str(pca3.explained_variance_ratio_))

# Print the cummulative sum of all ratios
print("\nCummulative sum of all ratios :\n" + str(pca3.explained_variance_ratio_.cumsum()))

# Show the data frame
pca3_df

# Plot the data points in the 3-dimensional space with feature pca_1, pca_2 and pca_3, using the class information in kmeans_model.labels_
plt.figure(figsize=(13,13))
ax = plt.subplot(projection="3d")
ax.scatter(pca3_df.pca_1, pca3_df.pca_2, pca3_df.pca_3, c=kmeans_model.labels_, cmap="rainbow", edgecolor='black')
ax.set_xlabel('PCA Component 1')
ax.set_ylabel('PCA Component 2')
ax.set_zlabel('PCA Component 3')
plt.title('Cluster Visualization using PCA (3D)')
plt.show()

"""###2-dimensional PCA"""

# We test the two most important principal components as they cover 47.6% (0.476) of the data variance.
pca2 = PCA(n_components=2)

# Fit (i.e., construct the 2-dimensional PCA space) and transform (i.e., project the original data points into the 2-dimensional PCA space).
pca2_data = pca2.fit_transform(scaled_data)

# Create a dataframe from the projected data points in 2-dimensional PCA space, namely "pca_1" and "pca_2"
pca2_df = pd.DataFrame(pca2_data, columns=["pca_1", "pca_2"])

# Print the amount of variance carried in each principal components (eigenvalues). This is the eigen values.
print("Amount of variance carried in each principal components (eigen values) :\n" + str(pca2.explained_variance_))

# Print the ratio of ammount of variance carried in each principal components (eigen values). The ratio is calculated by dividing the eigenvalue of each component by the sum of eigenvalues
print("\nRatio of variance carried in each principal components (eigen values) :\n" + str(pca2.explained_variance_ratio_))

# Print the cummulative sum of all ratios
print("\nCummulative sum of all ratios :\n" + str(pca2.explained_variance_ratio_.cumsum()))

pca2_df

# Plot the data points in the 2-dimensional space with feature pca_1 and pca_2, using the class information in kmeans_model.labels_
plt.figure(figsize=(12, 7))
ax = plt.subplot()
ax.scatter(pca2_df.pca_1, pca2_df.pca_2, c=kmeans_model.labels_, cmap="rainbow", edgecolor='black')
plt.show()

# Visualize the cluster with legend
plt.figure(figsize=(12, 7))
ax = plt.subplot()

# List of labels
group = kmeans_model.labels_

# Plot data samples in each label group: 0, 1 and 2.
for g in np.unique(group):
    index = np.where(group==g)     # Get all indices of label group g
    ax.scatter(pca2_df.iloc[index].pca_1, pca2_df.iloc[index].pca_2, label=g, cmap="rainbow",edgecolor='black')     # Plot data points with label
ax.legend()
plt.show()

"""##Cluster visualization with T-distributed Stochastic Neighbor Embedding (T-SNE)

### 3-dimensional
"""

# Import AgglomerativeClustering from scikit-learn
from sklearn.cluster import AgglomerativeClustering

# Run the hierarchical clustering algorithm with
clustering = AgglomerativeClustering(n_clusters=4, linkage="ward").fit(scaled_data) # Perform the cut at 4 clusters
clustering.labels_

# Import TSNE library
from sklearn.manifold import TSNE

# Perform cluster visualization using t-SNE for dimensionality reduction
tsne = TSNE(n_components=3, verbose=1, random_state=42)
tsne_data_3 = tsne.fit_transform(scaled_data)

# Create a data frame with TSNE features and data label
tsne3_df = pd.DataFrame()
tsne3_df["label"] = clustering.labels_
tsne3_df["feature-1"] = tsne_data_3[:,0]
tsne3_df["feature-2"] = tsne_data_3[:,1]
tsne3_df["feature-3"] = tsne_data_3[:,2]  # Add the third dimension

# Visualize the clusters
from mpl_toolkits.mplot3d import Axes3D  # Import Axes3D for 3D plotting
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(13, 13))
ax = fig.add_subplot(111, projection='3d')  # Create 3D subplot

# Scatter plot
scatter = ax.scatter(tsne3_df["feature-1"], tsne3_df["feature-2"], tsne3_df["feature-3"], c=tsne3_df["label"], cmap="viridis", alpha=0.6)
ax.set_title('Cluster visualization in 3D space')
ax.set_xlabel('Feature 1')
ax.set_ylabel('Feature 2')
ax.set_zlabel('Feature 3')
plt.legend(*scatter.legend_elements(), title="Clusters")
plt.show()

"""### 2-dimensional"""

# Import TSNE library
from sklearn.manifold import TSNE

# Perform cluster visualization using t-SNE for dimensionality reduction
tsne_2 = TSNE(n_components=2, verbose=1, random_state=42)
tsne_data_2 = tsne_2.fit_transform(scaled_data)

# Create a data frame with TSNE features and data label
tsne2_df = pd.DataFrame()
tsne2_df["label"] = clustering.labels_
tsne2_df["feature-1"] = tsne_data_2[:,0]
tsne2_df["feature-2"] = tsne_data_2[:,1]

# Visualize the clusters
sns.scatterplot(x="feature-1", y="feature-2", # Set the two TSNE features to the axes
                hue=tsne2_df.label.tolist(), # Set the label
                palette=sns.color_palette("husl", 4), # Set cluster colors
                data=tsne2_df, alpha=0.5).set(title="Cluster visualization on a 2-dimensional space") # Set dataframe to visulize and plot title

"""#Take some samples from each cluster and analyze their features to see the clusters' characteristics"""

import plotly.express as px

clusters = pd.DataFrame(scaled_data, columns = data.columns)
clusters['label'] = kmeans_model.labels_

polar = clusters.groupby("label").mean().reset_index()
polar = pd.melt(polar, id_vars = ["label"])

fig = px.line_polar(polar, r = "value", theta = "variable", color = "label", line_close = True, height = 700, width = 1000, symbol='label')

fig.show()

import plotly.graph_objects as go
import pandas as pd

# Compute describe statistics for each cluster
cluster_stats = data.groupby(kmeans_model.labels_).describe().transpose()

# Create a table using Plotly
fig = go.Figure(data=[go.Table(
    header=dict(values=['Feature', 'Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3']),
    cells=dict(values=[cluster_stats.index] + [cluster_stats[i].values.tolist() for i in range(4)])
)])

# Customize table layout
fig.update_layout(
    title='Cluster Statistics',
    autosize=True,
)

fig.show()

# 2. Visualize Cluster Samples
# Set up the figure and axes
fig, axes = plt.subplots(5, 4, figsize=(20, 20))

# Flatten the axes array for easy iteration
axes = axes.flatten()

# Iterate over each feature and plot the boxplot for each cluster
for i, feature in enumerate(data.columns):
    sns.boxplot(x=kmeans_model.labels_, y=data[feature], ax=axes[i])
    axes[i].set_title(f"{feature} Distribution by Cluster")

# Adjust layout
plt.tight_layout()
plt.show()

# Let's analyze some samples from each cluster
for i in range(4):
    cluster_samples = data[kmeans_model.labels_ == i].sample(3)  # Sample 5 data points from each cluster
    print(f"Cluster {i} samples:")
    print(cluster_samples)
    print("\n")

"""## **Some observations and analysis**
**1. Cluster 0:**

* BALANCE: The sampled data from Cluster 0 shows a relatively high balance amount left in their accounts for making purchases.
* PURCHASES: The purchases made by users in this cluster are relatively high.
* CASH_ADVANCE: Users in this cluster have a moderate level of cash advance.
* CREDIT_LIMIT: The credit limit for users in this cluster is relatively high.
* TENURE: The tenure for users in this cluster is around 12 months.

Based on these observations, the samples from Cluster 0 seem appropriate for this cluster as they exhibit characteristics similar to the overall distribution of features described for this cluster.

**2. Cluster 1:**

* BALANCE: The balance in this cluster is relatively lower compared to other clusters.
* PURCHASES: Users in this cluster have low purchase amounts.
* CASH_ADVANCE: Users in this cluster have a higher frequency of cash advances compared to other clusters.
* CREDIT_LIMIT: The credit limit for users in this cluster is relatively low compared to other clusters.
* TENURE: The tenure for users in this cluster is around 12 months, similar to other clusters.

Based on these observations, the samples from Cluster 1 seem appropriate for this cluster as they exhibit characteristics similar to the overall distribution of features described for this cluster.

**3. Cluster 2:**

* BALANCE: The balance in this cluster is relatively low, which aligns with the description.
* PURCHASES: Users in this cluster have relatively low purchase amounts.
* CASH_ADVANCE: Users in this cluster have a low frequency of cash advances.
* CREDIT_LIMIT: The credit limit for users in this cluster is relatively low.
* TENURE: The tenure for users in this cluster is around 12 months.

Based on these observations, the samples from Cluster 2 seem appropriate for this cluster as they exhibit characteristics similar to the overall distribution of features described for this cluster.

**3. Cluster 3:**

* BALANCE: The balance in this cluster is relatively low.
* PURCHASES: Users in this cluster have relatively low purchase amounts.
* CASH_ADVANCE: Users in this cluster have a low frequency of cash advances.
* CREDIT_LIMIT: The credit limit for users in this cluster is relatively low.
* TENURE: The tenure for users in this cluster is around 12 months, similar to other clusters.

Based on these observations, the samples from Cluster 3 seem appropriate for this cluster as they exhibit characteristics similar to the overall distribution of features described for this cluster.

***--> Overall, the sampled data from each cluster appears to be appropriate for their respective clusters based on the characteristics observed.***
"""

#The Silhouette Score is a metric used to evaluate the quality of clustering algorithms.
# It measures how similar an object is to its own cluster compared to other clusters.
# The score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.

# Analyze cluster characteristics
from sklearn.metrics import silhouette_score

# Evaluate clustering performance using silhouette score
silhouette_avg = silhouette_score(scaled_data, kmeans_model.labels_)
print(f"Silhouette Score: {silhouette_avg}")

"""**Some interpretation:**

In this case, the Silhouette Score of 0.1976 indicates that the clustering algorithm has produced clusters that are moderately well-separated. It suggests that the objects within each cluster are relatively close to each other and far from objects in neighboring clusters, but there is still some overlap or ambiguity in the clustering.

Overall, a Silhouette Score of 0.1976 suggests that the clustering algorithm has produced reasonable clusters, but there may be room for improvement in cluster separation.
"""